{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GCP_BQ_CREDENTIALS_FILE_PATH = os.environ['GCP_BQ_CREDENTIALS']\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    GCP_BQ_CREDENTIALS_FILE_PATH, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id,)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "first_in_year_tables = [\n",
    "    'httparchive.summary_requests.2018_01_01_mobile',\n",
    "    'httparchive.summary_requests.2019_02_01_mobile',\n",
    "    'httparchive.summary_requests.2020_01_01_mobile',\n",
    "    'httparchive.summary_requests.2021_02_01_mobile',\n",
    "    'httparchive.summary_requests.2022_01_01_mobile',\n",
    "    'httparchive.summary_requests.2023_01_01_mobile'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "query_string = r'''\n",
    "WITH final_extracted_table AS (\n",
    "  WITH rt_extracted_values_table AS (\n",
    "    WITH nel_values_extracted_table AS (\n",
    "      WITH nel_extracted_table AS (\n",
    "        WITH joined_table AS (\n",
    "          WITH filtered_table AS (\n",
    "            SELECT\n",
    "            MIN(requestid) min_req_id,\n",
    "            #COUNTIF(firstReq = false) occurences_count_not_firstreq,\n",
    "            REGEXP_EXTRACT(url, r\"http[s]?:[\\/][\\/]([^\\/:]+)\") AS url_domain,\n",
    "            FROM `%s`\n",
    "            GROUP BY url_domain\n",
    "          )\n",
    "          SELECT\n",
    "          requestid,\n",
    "          LOWER(respOtherHeaders) resp_headers,\n",
    "          status,\n",
    "          url,\n",
    "          type,\n",
    "          ext,\n",
    "          firstReq,\n",
    "          FROM filtered_table\n",
    "          INNER JOIN `%s` ON filtered_table.min_req_id = requestid\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "        requestid,\n",
    "        type,\n",
    "        ext,\n",
    "        firstReq,\n",
    "        status,\n",
    "        url,\n",
    "        resp_headers,\n",
    "\n",
    "        (SELECT COUNT(*) FROM joined_table) AS unique_domain_count_before_filtration,\n",
    "        (SELECT COUNT(*) FROM joined_table WHERE firstReq = true) AS unique_domain_firstreq_count_before_filtration,\n",
    "        #REGEXP_EXTRACT(url, r\"http[s]?:[\\/][\\/]([^\\/:]+)\") AS url_domain,\n",
    "        REGEXP_CONTAINS(resp_headers, r\"(?:^|.*[\\s,]+)(nel\\s*[=]\\s*)\") AS contains_nel,\n",
    "        REGEXP_EXTRACT(resp_headers, r\"(?:^|.*[\\s,]+)nel\\s*[=]\\s*({.*?})\") AS nel_value,\n",
    "\n",
    "        FROM joined_table\n",
    "      )\n",
    "      SELECT\n",
    "      requestid,\n",
    "      type,\n",
    "      ext,\n",
    "      firstReq,\n",
    "      status,\n",
    "      url,\n",
    "      #url_domain,\n",
    "      unique_domain_count_before_filtration,\n",
    "      unique_domain_firstreq_count_before_filtration,\n",
    "      contains_nel,\n",
    "\n",
    "      nel_value,\n",
    "      resp_headers, # debug rm, todo: zceknout jestli jsou vsechny co rikaji ze contains_nel maji values\n",
    "\n",
    "      # extract nel values\n",
    "      REGEXP_EXTRACT(nel_value, r\".*max_age[\\\"\\']\\s*:\\s*([0-9]+)\") AS nel_max_age,\n",
    "      REGEXP_EXTRACT(nel_value, r\".*failure[_]fraction[\\\"\\']\\s*:\\s*([0-9\\.]+)\") AS nel_failure_fraction,\n",
    "      REGEXP_EXTRACT(nel_value, r\".*success[_]fraction[\\\"\\']\\s*:\\s*([0-9\\.]+)\") AS nel_success_fraction,\n",
    "      REGEXP_EXTRACT(nel_value, r\".*include[_]subdomains[\\\"\\']\\s*:\\s*(\\w+)\") AS nel_include_subdomains,\n",
    "\n",
    "      REGEXP_EXTRACT(nel_value, r\".*report_to[\\\"\\']\\s*:\\s*[\\\"\\'](.+?)[\\\"\\']\") AS nel_report_to_group,\n",
    "\n",
    "      FROM nel_extracted_table\n",
    "      WHERE contains_nel = true\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "    requestid,\n",
    "    type,\n",
    "    ext,\n",
    "    firstReq,\n",
    "    status,\n",
    "    url,\n",
    "    unique_domain_count_before_filtration,\n",
    "    unique_domain_firstreq_count_before_filtration,\n",
    "    contains_nel,\n",
    "    nel_max_age,\n",
    "    nel_failure_fraction,\n",
    "    nel_success_fraction,\n",
    "    nel_include_subdomains,\n",
    "    nel_report_to_group,\n",
    "    #nel_value, # debug rm\n",
    "    #reportto_value, # debug rm\n",
    "    resp_headers,\n",
    "\n",
    "    (SELECT COUNT(*) FROM nel_values_extracted_table) AS nel_count_before_filtration,\n",
    "\n",
    "    REGEXP_EXTRACT(resp_headers, CONCAT(r\"report[-]to\\s*?[=].*([{](?:(?:[^\\{]*?endpoints.*?[\\[][^\\[]*?[\\]][^\\}]*?)|(?:[^\\{]*?endpoints.*?[\\{][^\\{]*?[\\}]))?[^\\]\\}]*?group[\\'\\\"][:]\\s*?[\\'\\\"]\", nel_report_to_group, r\"(?:(?:[^\\}]*?endpoints[^\\}]*?[\\[][^\\[]*?[\\]][^\\{]*?)|(?:[^\\}]*?endpoints.*?[\\{][^\\{]*?[\\}]))?.*?[}])\")) AS rt_value,\n",
    "\n",
    "    FROM nel_values_extracted_table\n",
    "  )\n",
    "  SELECT\n",
    "  requestid,\n",
    "  type,\n",
    "  ext,\n",
    "  firstReq,\n",
    "  status,\n",
    "  url,\n",
    "  unique_domain_count_before_filtration,\n",
    "  unique_domain_firstreq_count_before_filtration,\n",
    "  contains_nel,\n",
    "  nel_max_age,\n",
    "  nel_failure_fraction,\n",
    "  nel_success_fraction,\n",
    "  nel_include_subdomains,\n",
    "  nel_report_to_group,\n",
    "  #nel_value, # debug rm\n",
    "  rt_value,\n",
    "  #resp_headers, # debug rm, todo: zceknout jestli jsou vsechny co rikaji ze contains_nel maji values\n",
    "  nel_count_before_filtration,\n",
    "\n",
    "  REGEXP_EXTRACT(rt_value, r\".*group[\\\"\\']\\s*:\\s*[\\\"\\'](.+?)[\\\"\\']\") AS rt_group,\n",
    "\n",
    "  REGEXP_EXTRACT_ALL(rt_value, r\"url[\\\"\\']\\s*:\\s*[\\\"\\']http[s]?:[\\\\]*?[\\/][\\\\]*?[\\/]([^\\/]+?)[\\\\]*?[\\/\\\"]\") AS rt_endpoints,\n",
    "  REGEXP_EXTRACT(rt_value, r\"url[\\\"\\']\\s*:\\s*[\\\"\\']http[s]?:[\\\\]*?[\\/][\\\\]*?[\\/]([^\\/]+?)[\\\\]*?[\\/\\\"]\") AS rt_url,\n",
    "  REGEXP_EXTRACT(rt_value, r\"url[\\\"\\']\\s*:\\s*(?:[\\\"\\']http[s]?:[\\\\]*?[\\/][\\\\]*?[\\/].*?([^\\.]+?[.][^\\.]+?)[\\\\]*?[\\/\\\"])\") AS rt_url_sld,\n",
    "\n",
    "  FROM rt_extracted_values_table\n",
    ")\n",
    "\n",
    "SELECT\n",
    "requestid,\n",
    "type,\n",
    "ext,\n",
    "firstReq,\n",
    "status,\n",
    "url,\n",
    "unique_domain_count_before_filtration,\n",
    "unique_domain_firstreq_count_before_filtration,\n",
    "contains_nel,\n",
    "nel_max_age,\n",
    "nel_failure_fraction,\n",
    "nel_success_fraction,\n",
    "nel_include_subdomains,\n",
    "nel_report_to_group,\n",
    "#nel_value, # debug rm\n",
    "#rt_value, # debug rm\n",
    "#resp_headers, # debug rm, todo: zceknout jestli jsou vsechny co rikaji ze contains_nel maji values\n",
    "nel_count_before_filtration,\n",
    "rt_group,\n",
    "rt_endpoints,\n",
    "rt_url,\n",
    "rt_url_sld,\n",
    "\n",
    "FROM final_extracted_table\n",
    "# filter data to only those that we could parse, and that has both report_to and group matching\n",
    "# by analysis, the filtered out records contains either not json value, bad formating such as \\\" for json quotes, or are maybe improperly parsed by previous processing by table creators (no value, missing brackets)\n",
    "WHERE nel_report_to_group = rt_group and nel_report_to_group is not null and rt_group is not null;\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def processing_bytes_estimation(table_list, query_string):\n",
    "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
    "    sum_mb = 0\n",
    "\n",
    "    for table_name in table_list:\n",
    "        query_job = client.query(\n",
    "            (\n",
    "                query_string % (table_name, table_name)\n",
    "            ),\n",
    "            job_config=job_config,\n",
    "        )\n",
    "        processed_mb = query_job.total_bytes_processed/1024/1024\n",
    "        print(\"This query '{}' will process {} MB, {} GB.\".format(table_name, processed_mb, processed_mb/1024))\n",
    "        sum_mb += processed_mb\n",
    "\n",
    "    print()\n",
    "    print(\"Total will process {} MB, {} GB, {} TB\".format(sum_mb, sum_mb/1024, sum_mb/1024/1024))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "processing_bytes_estimation(first_in_year_tables, query_string)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def run_queries_store_data(table_list, query_string) -> list:\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_list = []\n",
    "\n",
    "    for table_name in table_list:\n",
    "        query_job = client.query(\n",
    "            (\n",
    "                query_string % (table_name, table_name)\n",
    "            ),\n",
    "            job_config=job_config,\n",
    "        )\n",
    "\n",
    "        dst_tmp_table_dict = query_job.__dict__['_properties']['configuration']['query']['destinationTable']\n",
    "        dst_tmp_table_name = f'{dst_tmp_table_dict[\"projectId\"]}.{dst_tmp_table_dict[\"datasetId\"]}.{dst_tmp_table_dict[\"tableId\"]}'\n",
    "\n",
    "        tmp_df = query_job.to_dataframe()\n",
    "\n",
    "        tmp_df.to_parquet(f\"results_desktop/{table_name}.{dst_tmp_table_name}.parquet\")\n",
    "        job_list.append(query_job)\n",
    "\n",
    "    return job_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "job_list = run_queries_store_data(first_in_year_tables, query_string)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(job_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(\"asdf\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
