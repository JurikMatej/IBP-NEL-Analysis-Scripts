#!/usr/bin/env python3
"""
author:         Matej Jurík <xjurik12@stud.fit.vutbr.cz>

description:    Query the Google Cloud Platform's BigQuery for HTTP Archive datasets containing summary of requests
                and their responses generated by crawling the internet.
                The specific query being run here extracts all responses that returned with a NEL header.

                This script acts as the updated version of the former script created by Kamil Jeřábek
                (original repo: from https://github.com/kjerabek/nel-http-archive-analysis).
                The necessity of this script comes from the need to transfer a large volume of data from BigQuery.
                The former script somehow managed to query the HTTP Archive datasets via the base interface
                (google.cloud.bigquery API) without reaching its limits for the maximum size of each query results.
                Here, the BigQuery Storage API (google.cloud.bigquery_storage) is used to retrieve large volume of data.

purpose:        Get raw data from the Google Cloud and store as an apache parquet file in order for it to be available
                locally for further processing.
                (IMPORTANT: do not modify the downloaded data itself - additional queries cost money)
"""

import json
import os
import pathlib
import time

import google.api_core.exceptions
import pandas as pd
import pyarrow.parquet as pq
from google.cloud import bigquery
from google.cloud import storage
from google.cloud.bigquery_storage import BigQueryReadClient
from google.cloud.bigquery_storage_v1 import types
from google.oauth2 import service_account

from src.bq_parametrized_queries import QUERY_NEL_DATA


###############################
# CONFIGURE THESE BEFORE USE: #
###############################
GC_PROJECT_NAME = 'nel-analysis'
GC_DATASET_NAME = 'httparchive_fetch_temp'
GC_TABLE_NAME = 'nel_data'

GC_TABLE_LOCATION = "US"

GC_PATH_TO_CREDENTIALS_FILE = "gcp-secrets/nel-analysis-f1c127130c7f-nel-analysis-admin.json"

DATA_EXPORT_BUCKET_NAME = "downloadable-nel-analysis-data"

DATA_PREPARATION_NEL_DATA_QUERY = QUERY_NEL_DATA

DOWNLOAD_OUTPUT_DIR_PATH = "httparchive_data_raw"
DOWNLOAD_TEMP_DIR_PATH = f"{DOWNLOAD_OUTPUT_DIR_PATH}/blobs"
DOWNLOAD_CONFIG_PATH = "download_config.json"

# TODO improve logging to match the download process for MANY tables, not only ONE = more concise & informative


def prepare_nel_data_table(http_archive_table_name: str):
    """
    Prepare required infrastructure inside BigQuery.
    Create a temporary dataset, a temporary table inside of it and eventually also query HTTP Archive data into it.
    After this function completes successfully, the requested table is available at BigQuery to be downloaded

    TODO(refactor) remove the population step from this function.
                   Structure is first to be created before even taking any source tables into account
                   and then populated iteratively with source tables of custom selection
    """
    print("##########...Preparing temporary infrastructure for NEL analysis data...##########")

    # Build a BigQuery infrastructure administration client instance
    client = _bq_infrastructure_administration_client()

    # Prepare infrastructure for working NEL data
    _create_temp_dataset(client)
    _create_temp_table(client)

    # Populate the prepared table with NEL data
    _populate_temp_table(client, http_archive_table_name)


def _bq_infrastructure_administration_client() -> bigquery.Client:
    """
    Create a simple BigQuery client able to handle infrastructure administration.
    Note that the client needs valid credentials with just enough privileges to create datasets,
    tables and execute query jobs.

    :return: BigQuery client meant to handle infrastructure administration
    """
    credentials = service_account.Credentials.from_service_account_file(
        GC_PATH_TO_CREDENTIALS_FILE,
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )
    client = bigquery.Client(credentials=credentials, project=GC_PROJECT_NAME)

    return client


def _create_temp_dataset(client):
    """
    Creates a BigQuery dataset with a pre-configured name (see global config variables - {GC_DATASET_NAME})
    """
    dataset_id = f"{client.project}.{GC_DATASET_NAME}"
    dataset = bigquery.Dataset(dataset_id)

    one_day_in_ms = 1000 * 60 * 60 * 24

    dataset.location = "US"
    dataset.description = "Dataset for NEL Analysis data"
    dataset.default_table_expiration_ms = one_day_in_ms

    try:
        dataset = client.create_dataset(dataset, timeout=30)
        print(f"Created DATASET '{client.project}.{dataset.dataset_id}'")

    except google.api_core.exceptions.Conflict:
        print(f"DATASET '{client.project}.{dataset.dataset_id}' already exists. Proceeding to use it")


def _create_temp_table(client: bigquery.Client):
    """
    Create a temporary BigQuery table for storing intermediate query results of data for the NEL analysis.
    If the temporary table already exists, delete all it's contents before using it.

    NOTE: It is required, that this method is called after the dataset of name {GC_DATASET_NAME}
          has already been created

    :param client: Basic Google Cloud BigQuery API client with credentials and project ID
                   already provided
    """
    table_id = f"{GC_PROJECT_NAME}.{GC_DATASET_NAME}.{GC_TABLE_NAME}"

    nel_data_schema = [
        bigquery.SchemaField("requestId", "INTEGER"),
        bigquery.SchemaField("firstReq", "BOOLEAN"),
        bigquery.SchemaField("type", "STRING"),
        bigquery.SchemaField("ext", "STRING"),
        bigquery.SchemaField("url", "STRING"),
        bigquery.SchemaField("status", "INTEGER"),
        bigquery.SchemaField("total_crawled_resources", "INTEGER"),
        bigquery.SchemaField("total_crawled_domains", "INTEGER"),
        bigquery.SchemaField("nel_max_age", "STRING"),
        bigquery.SchemaField("nel_failure_fraction", "STRING"),
        bigquery.SchemaField("nel_success_fraction", "STRING"),
        bigquery.SchemaField("nel_include_subdomains", "STRING"),
        bigquery.SchemaField("nel_report_to", "STRING"),
        bigquery.SchemaField("total_crawled_resources_with_nel", "INTEGER"),
        bigquery.SchemaField("rt_group", "STRING"),
        bigquery.SchemaField("rt_collectors", "STRING", mode="REPEATED"),
    ]

    table = bigquery.Table(table_id, schema=nel_data_schema)

    try:
        table = client.create_table(table)  # Make an API request.
        print(f"Created temporary TABLE '{table.project}.{table.dataset_id}.{table.table_id}'")

    except google.api_core.exceptions.Conflict:
        print(f"Temporary TABLE '{table.project}.{table.dataset_id}.{table.table_id}' already exists")

        # NOTE: billing needs to be enabled for the project in order for the DELETE OP to work
        delete_table_contents_query = f"DELETE FROM `{table_id}` WHERE 1=1;"
        client.query_and_wait(delete_table_contents_query)
        print("Temporary TABLE contents deleted. Proceeding to use it")


def _populate_temp_table(client: bigquery.Client, http_archive_table_name: str):
    """
    Populate the temporary BigQuery table (created by this script) with data fetched from the configured
    HTTP Archive table using the query string provided as a variable in the global scope of this script.

    :param client: Basic Google Cloud BigQuery API client with credentials and project ID
                   already provided
    """
    target_table_id = f"{GC_PROJECT_NAME}.{GC_DATASET_NAME}.{GC_TABLE_NAME}"
    job_config = bigquery.QueryJobConfig(destination=target_table_id)

    client.query_and_wait(DATA_PREPARATION_NEL_DATA_QUERY % http_archive_table_name, job_config=job_config)

    print(f"Loaded NEL analysis data from '{http_archive_table_name}' "
          f"to the temporary table '{target_table_id}'")


def _bq_storage_read_session_client():
    """
    Create a BigQuery Storage API client able to read large sum of data from BigQuery tables
    """
    return BigQueryReadClient.from_service_account_json(GC_PATH_TO_CREDENTIALS_FILE)


def delete_nel_data_table():
    print("####################...Deleting NEL data...####################")
    raise NotImplementedError()


def export_nel_data_table(exported_table_name: str):
    print("####################...Extracting NEL data...####################")
    client = _bq_infrastructure_administration_client()

    destination_uri = "gs://{}/{}".format(DATA_EXPORT_BUCKET_NAME,
                                          f"{exported_table_name}-*.parquet.snappy")
    dataset_ref = bigquery.DatasetReference(GC_PROJECT_NAME, GC_DATASET_NAME)
    table_ref = dataset_ref.table(GC_TABLE_NAME)

    job_config = bigquery.job.ExtractJobConfig()
    job_config.destination_format = bigquery.DestinationFormat.PARQUET
    job_config.compression = bigquery.Compression.SNAPPY

    extract_job = client.extract_table(
        table_ref,
        destination_uri,
        location=GC_TABLE_LOCATION,
        job_config=job_config,
    )

    extract_job.result()  # Waits for job to complete.

    print("NEL data exported successfully and is ready to be downloaded")


def download_exported_nel_data(exported_table_name: str):
    print("####################...Downloading exported NEL data...####################")
    storage_client = storage.Client.from_service_account_json(GC_PATH_TO_CREDENTIALS_FILE)
    bucket = storage_client.get_bucket(DATA_EXPORT_BUCKET_NAME)
    blobs = bucket.list_blobs(prefix=exported_table_name)

    dl_time = time.time()

    # TODO make a temporary dir and create only once
    pathlib.Path(DOWNLOAD_TEMP_DIR_PATH).mkdir(parents=True, exist_ok=True)

    for blob in blobs:
        destination_uri = '{}/{}'.format(DOWNLOAD_TEMP_DIR_PATH, blob.name)
        blob.download_to_filename(destination_uri)

    print("Exported NEL data downloaded successfully")
    print(f"Download time: {time.time() - dl_time}")


def gather_downloaded_nel_data_files(result_data_file_name: str):
    print("####################...Gathering downloaded NEL data into a single file...####################")
    gather_time = time.time()

    blob_dir = pathlib.Path(DOWNLOAD_TEMP_DIR_PATH)
    files = list(blob_dir.glob('*.parquet.snappy'))

    if len(files) < 1:
        print("Download completed with 0 files downloaded, aborting gathering downloaded NEL data into a single file")
        return

    schema = pq.ParquetFile(files[0]).schema_arrow

    result_path = f"{DOWNLOAD_OUTPUT_DIR_PATH}/{result_data_file_name}.parquet"
    with pq.ParquetWriter(result_path, schema=schema) as writer:
        for parquet_file in files:
            writer.write_table(pq.read_table(parquet_file, schema=schema))

    print()
    print(f"Result filesize: {os.path.getsize(result_path) / 2 ** 30} GB")
    print(f"Gather time time: {time.time() - gather_time}")


def delete_exported_nel_data(original_source_table_name: str):
    print("####################...Deleting exported NEL data...####################")
    storage_client = storage.Client.from_service_account_json(GC_PATH_TO_CREDENTIALS_FILE)
    bucket = storage_client.get_bucket(DATA_EXPORT_BUCKET_NAME)
    blobs = bucket.list_blobs(prefix=original_source_table_name)

    for blob in blobs:
        blob.delete()

    print("Exported NEL data deleted successfully")


def main():
    with open(DOWNLOAD_CONFIG_PATH, 'r') as config_file:
        download_conf = json.loads(config_file.read())

        for item in download_conf:
            for input_type in ('desktop', 'mobile'):
                httparchive_table_names = item.get(f'input_{input_type}')

                for httparchive_table_name in httparchive_table_names:

                    file_to_download_path = pathlib.Path(f"{DOWNLOAD_OUTPUT_DIR_PATH}/{httparchive_table_name}.parquet")
                    if file_to_download_path.is_file():
                        print(f"Table {httparchive_table_name} already among downloaded files")
                        continue

                    prepare_nel_data_table(httparchive_table_name)
                    print()

                    export_nel_data_table(httparchive_table_name)
                    print()

                    download_exported_nel_data(httparchive_table_name)
                    print()

                    gather_downloaded_nel_data_files(httparchive_table_name)
                    print()

                    delete_exported_nel_data(httparchive_table_name)
                    print()


if '__main__' == __name__:
    main()
