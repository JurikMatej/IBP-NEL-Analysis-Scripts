#!/usr/bin/env python3
"""
author:         Matej Jurík <xjurik12@stud.fit.vutbr.cz>

description:    Query the Google Cloud Platform's BigQuery for HTTP Archive datasets containing summary of requests
                and their responses generated by crawling the internet.
                The specific query being run here extracts all responses that returned with a NEL header.

                This script acts as the updated version of the former script created by Kamil Jeřábek
                (original repo: from https://github.com/kjerabek/nel-http-archive-analysis).
                The necessity of this script comes from the need to transfer a large volume of data from BigQuery.
                The former script somehow managed to query the HTTP Archive datasets via the base interface
                (google.cloud.bigquery API) without reaching its limits for the maximum size of each query results.
                Here, the BigQuery Storage API (google.cloud.bigquery_storage) is used to retrieve large volume of data.

purpose:        Get raw data from the Google Cloud and store as an apache parquet file in order for it to be available
                locally for further processing.
                (IMPORTANT: do not modify the downloaded data itself - additional queries cost money)
"""

import json
import os
import pathlib
import time
from typing import List

import google.api_core.exceptions
import pyarrow.parquet as pq
from google.cloud import bigquery
from google.cloud import storage
from google.cloud.bigquery_storage import BigQueryReadClient
from google.oauth2 import service_account

from src.bq_parametrized_queries import \
    QUERY_NEL_DATA_2_DESKTOP_2_MOBILE, \
    QUERY_NEL_DATA_2_DESKTOP_1_MOBILE, \
    QUERY_NEL_DATA_1_DESKTOP_1_MOBILE, \
    QUERY_NEL_DATA_1_DESKTOP


###############################
# CONFIGURE THESE BEFORE USE: #
###############################
GC_PROJECT_NAME = 'nel-analysis'
GC_DATASET_NAME = 'httparchive_fetch_temp'
GC_TABLE_NAME = 'nel_data'

GC_TABLE_LOCATION = "US"

GC_PATH_TO_CREDENTIALS_FILE = "gcp-secrets/nel-analysis-f1c127130c7f-nel-analysis-admin.json"

DATA_EXPORT_BUCKET_NAME = "downloadable-nel-analysis-data"

DOWNLOAD_OUTPUT_DIR_PATH = "httparchive_data_raw"
DOWNLOAD_TEMP_DIR_PATH = f"{DOWNLOAD_OUTPUT_DIR_PATH}/blobs"
DOWNLOAD_CONFIG_PATH = "download_config.json"

# TODO improve logging to match the download process for MANY tables, not only ONE = more concise & informative


def prepare_nel_data_table():
    """
    Prepare required infrastructure inside BigQuery.
    Create a temporary dataset, a temporary table inside of it and eventually also query HTTP Archive data into it.
    After this function completes successfully, the requested table is available at BigQuery to be downloaded
    """
    print("#..Preparing temporary infrastructure for NEL analysis data...#")

    # Build a BigQuery infrastructure administration client instance
    client = _bq_infrastructure_administration_client()

    # Prepare infrastructure for working NEL data
    _create_temp_dataset(client)
    _create_temp_table(client)


def _bq_infrastructure_administration_client() -> bigquery.Client:
    """
    Create a simple BigQuery client able to handle infrastructure administration.
    Note that the client needs valid credentials with just enough privileges to create datasets,
    tables and execute query jobs.

    :return: BigQuery client meant to handle infrastructure administration
    """
    credentials = service_account.Credentials.from_service_account_file(
        GC_PATH_TO_CREDENTIALS_FILE,
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )
    client = bigquery.Client(credentials=credentials, project=GC_PROJECT_NAME)

    return client


def _create_temp_dataset(client):
    """
    Creates a BigQuery dataset with a pre-configured name (see global config variables - {GC_DATASET_NAME})
    """
    dataset_id = f"{client.project}.{GC_DATASET_NAME}"
    dataset = bigquery.Dataset(dataset_id)

    one_day_in_ms = 1000 * 60 * 60 * 24

    dataset.location = "US"
    dataset.description = "Dataset for NEL Analysis data"
    dataset.default_table_expiration_ms = one_day_in_ms

    try:
        dataset = client.create_dataset(dataset, timeout=30)
        print(f"Created DATASET '{client.project}.{dataset.dataset_id}'")

    except google.api_core.exceptions.Conflict:
        print(f"DATASET '{client.project}.{dataset.dataset_id}' already exists. Proceeding to use it")


def _create_temp_table(client: bigquery.Client):
    """
    Create a temporary BigQuery table for storing intermediate query results of data for the NEL analysis.
    If the temporary table already exists, delete all it's contents before using it.

    NOTE: It is required, that this method is called after the dataset of name {GC_DATASET_NAME}
          has already been created

    :param client: Basic Google Cloud BigQuery API client with credentials and project ID
                   already provided
    """
    table_id = f"{GC_PROJECT_NAME}.{GC_DATASET_NAME}.{GC_TABLE_NAME}"

    nel_data_schema = [
        bigquery.SchemaField("requestId", "INTEGER"),
        bigquery.SchemaField("firstReq", "BOOLEAN"),
        bigquery.SchemaField("type", "STRING"),
        bigquery.SchemaField("ext", "STRING"),
        bigquery.SchemaField("url", "STRING"),
        bigquery.SchemaField("url_etld", "STRING"),
        bigquery.SchemaField("status", "INTEGER"),
        bigquery.SchemaField("total_crawled_resources", "INTEGER"),
        bigquery.SchemaField("total_crawled_domains", "INTEGER"),
        bigquery.SchemaField("nel_max_age", "STRING"),
        bigquery.SchemaField("nel_failure_fraction", "STRING"),
        bigquery.SchemaField("nel_success_fraction", "STRING"),
        bigquery.SchemaField("nel_include_subdomains", "STRING"),
        bigquery.SchemaField("nel_report_to", "STRING"),
        bigquery.SchemaField("total_crawled_resources_with_nel", "INTEGER"),
        bigquery.SchemaField("rt_group", "STRING"),
        bigquery.SchemaField("rt_collectors", "STRING", mode="REPEATED"),
    ]

    table = bigquery.Table(table_id, schema=nel_data_schema)

    try:
        table = client.create_table(table)  # Make an API request.
        print(f"Created temporary TABLE '{table.project}.{table.dataset_id}.{table.table_id}'")

    except google.api_core.exceptions.Conflict:
        print(f"Temporary TABLE '{table.project}.{table.dataset_id}.{table.table_id}' already exists")

        # NOTE: billing needs to be enabled for the project in order for the DELETE OP to work
        delete_table_contents_query = f"DELETE FROM `{table_id}` WHERE 1=1;"
        client.query_and_wait(delete_table_contents_query)
        print("Temporary TABLE contents deleted. Proceeding to use it")


def select_query_by_table_structure(desktop_table_list: List[str], mobile_table_list: List[str]) -> str:
    """
    Determine the query to be used based on how the monthly data tables are split.
    Currently, this method supports any kind of data table split for the time period from 2018/09 to 2024/01:
        * 2 desktop tables & 2 mobile tables
        * 2 desktop tables & 1 mobile table
        * 1 desktop table & 1 mobile table
        * 1 desktop table

    :param desktop_table_list: List of desktop table names to query for
    :param mobile_table_list: List of mobile table names to query for
    :return: Suitable query to fetch all tables provided in the parameters
    """
    if len(desktop_table_list) == 2 and len(mobile_table_list) == 2:
        return QUERY_NEL_DATA_2_DESKTOP_2_MOBILE % (
            desktop_table_list[1],
            desktop_table_list[1],
            desktop_table_list[0],
            desktop_table_list[0],
            mobile_table_list[1],
            mobile_table_list[1],
            mobile_table_list[0],
            mobile_table_list[0],
        )
    elif len(desktop_table_list) == 2 and len(mobile_table_list) == 1:
        return QUERY_NEL_DATA_2_DESKTOP_1_MOBILE % (
            desktop_table_list[1],
            desktop_table_list[1],
            desktop_table_list[0],
            desktop_table_list[0],
            mobile_table_list[0],
            mobile_table_list[0],
        )
    elif len(desktop_table_list) == 1 and len(mobile_table_list) == 1:
        return QUERY_NEL_DATA_1_DESKTOP_1_MOBILE % (
            desktop_table_list[0],
            desktop_table_list[0],
            mobile_table_list[0],
            mobile_table_list[0],
        )
    elif len(desktop_table_list) == 1 and len(mobile_table_list) == 0:
        return QUERY_NEL_DATA_1_DESKTOP % (
            desktop_table_list[0],
            desktop_table_list[0],
        )
    else:
        raise NotImplementedError("Other type of table fragmentation should not occur during the analyzed time period")


def populate_temp_table(client: bigquery.Client, query_string: str, output_filename: str):
    """
    Populate the temporary BigQuery table (created by this script) with data fetched from the configured
    HTTP Archive table using the query string provided as a variable in the global scope of this script.

    :param client: Basic Google Cloud BigQuery API client with credentials and project ID
                   already provided
    :param query_string: Query to use for table population
    :param output_filename: Name of the output file (here only for logging info)
    """
    print("####...Populating temporary table with NEL analysis data...####")

    target_table_id = f"{GC_PROJECT_NAME}.{GC_DATASET_NAME}.{GC_TABLE_NAME}"
    job_config = bigquery.QueryJobConfig(destination=target_table_id)

    client.query_and_wait(query_string, job_config=job_config)

    print(f"The temporary table {target_table_id} was loaded successfully with {output_filename}")


def _bq_storage_read_session_client():
    """
    Create a BigQuery Storage API client able to read large sum of data from BigQuery tables
    """
    return BigQueryReadClient.from_service_account_json(GC_PATH_TO_CREDENTIALS_FILE)


def export_nel_data_table(exported_table_name: str):
    print("###################...Extracting NEL data...###################")
    client = _bq_infrastructure_administration_client()

    destination_uri = "gs://{}/{}".format(DATA_EXPORT_BUCKET_NAME,
                                          f"{exported_table_name}-*.parquet.snappy")
    dataset_ref = bigquery.DatasetReference(GC_PROJECT_NAME, GC_DATASET_NAME)
    table_ref = dataset_ref.table(GC_TABLE_NAME)

    job_config = bigquery.job.ExtractJobConfig()
    job_config.destination_format = bigquery.DestinationFormat.PARQUET
    job_config.compression = bigquery.Compression.SNAPPY

    extract_job = client.extract_table(
        table_ref,
        destination_uri,
        location=GC_TABLE_LOCATION,
        job_config=job_config,
    )

    extract_job.result()  # Waits for job to complete.

    print("NEL data exported successfully and is ready to be downloaded")


def download_exported_nel_data(exported_table_name: str):
    print("##############...Downloading exported NEL data...##############")
    storage_client = storage.Client.from_service_account_json(GC_PATH_TO_CREDENTIALS_FILE)
    bucket = storage_client.get_bucket(DATA_EXPORT_BUCKET_NAME)
    blobs = bucket.list_blobs(prefix=exported_table_name)

    dl_time = time.time()

    for blob in blobs:
        destination_uri = '{}/{}'.format(DOWNLOAD_TEMP_DIR_PATH, blob.name)
        blob.download_to_filename(destination_uri)

    print("Exported NEL data downloaded successfully")
    print(f"Download time: {time.time() - dl_time}")


def gather_downloaded_nel_data_files(result_data_file_name: str):
    print("#####..Gathering downloaded NEL data into a single file...#####")
    gather_time = time.time()

    blob_dir = pathlib.Path(DOWNLOAD_TEMP_DIR_PATH)
    files = list(blob_dir.glob('*.parquet.snappy'))

    if len(files) < 1:
        print("Download completed with 0 files downloaded, aborting gathering downloaded NEL data into a single file")
        return

    schema = pq.ParquetFile(files[0]).schema_arrow

    result_path = f"{DOWNLOAD_OUTPUT_DIR_PATH}/{result_data_file_name}.parquet"
    with pq.ParquetWriter(result_path, schema=schema) as writer:
        for parquet_file in files:
            # Gather each standalone blob into a single file
            writer.write_table(pq.read_table(parquet_file, schema=schema))
            # Remove the blob after, so it does not get gathered to different data tables
            parquet_file.unlink()

    print()
    print(f"Result filesize: {os.path.getsize(result_path) / 2 ** 30} GB")
    print(f"Gather time: {time.time() - gather_time}")


def delete_exported_nel_data(original_source_table_name: str):
    print("################..Deleting exported NEL data...################")
    storage_client = storage.Client.from_service_account_json(GC_PATH_TO_CREDENTIALS_FILE)
    bucket = storage_client.get_bucket(DATA_EXPORT_BUCKET_NAME)
    blobs = bucket.list_blobs(prefix=original_source_table_name)

    for blob in blobs:
        blob.delete()

    print("Exported NEL data deleted successfully")


def delete_nel_data_table_contents(client: google.cloud.bigquery.Client):
    print("########..Deleting temporary NEL data table contents...########")

    table_id = f"{GC_PROJECT_NAME}.{GC_DATASET_NAME}.{GC_TABLE_NAME}"
    delete_table_contents_query = f"DELETE FROM `{table_id}` WHERE 1=1;"

    client.query_and_wait(delete_table_contents_query)
    print("Temporary table contents deleted")


def main():
    # Prepare BigQuery infrastructure for querying data
    query_client = _bq_infrastructure_administration_client()
    prepare_nel_data_table()
    print()

    # Prepare download infrastructure on this device
    pathlib.Path(DOWNLOAD_TEMP_DIR_PATH).mkdir(parents=True, exist_ok=True)

    # Run query & store
    with open(DOWNLOAD_CONFIG_PATH, 'r') as config_file:
        download_conf = json.loads(config_file.read())

        for item in download_conf:
            desktop_table_list = item.get('input_desktop', [])
            mobile_table_list = item.get('input_mobile', [])

            output_filename = item.get('processed_output', None)
            if output_filename is None:
                print(f"Tables with Desktop_1 {desktop_table_list[0]} - no output filename specified")
                continue

            file_to_download_path = pathlib.Path(f"{DOWNLOAD_OUTPUT_DIR_PATH}/{output_filename}.parquet")
            if file_to_download_path.is_file():
                print(f"Table {output_filename} already among downloaded files")
                continue

            query = select_query_by_table_structure(desktop_table_list, mobile_table_list)

            populate_temp_table(query_client, query, output_filename)
            print()

            export_nel_data_table(output_filename)
            print()

            download_exported_nel_data(output_filename)
            print()

            gather_downloaded_nel_data_files(output_filename)
            print()

            delete_exported_nel_data(output_filename)
            print()

            delete_nel_data_table_contents(query_client)
            print()


if '__main__' == __name__:
    main()
