#!/usr/bin/env python3
"""
author:         Matej Jurík <xjurik12@stud.fit.vutbr.cz>

description:    Query the Google Cloud Platform's BigQuery for HTTP Archive datasets containing summary of requests
                and their responses generated by crawling the internet.
                The specific query being run here extracts all responses that returned with a NEL header.

                This script acts as the updated version of the former script created by Kamil Jeřábek
                (original repo: from https://github.com/kjerabek/nel-http-archive-analysis).
                The necessity of this script comes from the need to transfer a large volume of data from BigQuery.
                The former script somehow managed to query the HTTP Archive datasets via the base interface
                (google.cloud.bigquery API) without reaching its limits for the maximum size of each query results.
                Here, the BigQuery Storage API (google.cloud.bigquery_storage) is used to retrieve large volume of data.

purpose:        Get raw data from the Google Cloud and store as an apache parquet file in order for it to be available
                locally for further processing.
                (IMPORTANT: do not modify the downloaded data itself - additional queries cost money)
"""
import os
from pathlib import Path

# TODO make download size estimates


import google.api_core.exceptions
import pandas as pd
import time
import pyarrow.parquet as pq
from google.cloud import bigquery
from google.cloud import storage
from google.cloud.bigquery_storage import BigQueryReadClient
from google.cloud.bigquery_storage_v1 import types
from google.oauth2 import service_account

from src.bq_parametrized_queries import QUERY_NEL_DATA

###############################
# CONFIGURE THESE BEFORE USE: #
###############################
GC_PROJECT_NAME = 'nel-analysis'
GC_DATASET_NAME = 'httparchive_fetch_temp'
GC_TABLE_NAME = 'nel_data'

GC_TABLE_LOCATION = "US"

GC_PATH_TO_CREDENTIALS_FILE = "gcp-secrets/nel-analysis-f1c127130c7f-nel-analysis-admin.json"

DATA_EXPORT_BUCKET_NAME = "downloadable-nel-analysis-data"

DATA_PREPARATION_NEL_DATA_QUERY = QUERY_NEL_DATA

# TODO(config) Create external config - TABLES_TO_DOWNLOAD > RESULT_TABLES_FOR_POST_PROCESS + ASSIGNED_PSL_FILE
# DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE = 'httparchive.summary_requests.2016_02_15_mobile'
# DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE = 'httparchive.summary_requests.2021_01_01_desktop'
DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE = 'httparchive.summary_requests.2020_02_01_desktop'


def prepare_nel_data_table():
    """
    Prepare required infrastructure inside BigQuery.
    Create a temporary dataset, a temporary table inside of it and eventually also query HTTP Archive data into it.
    After this function completes successfully, the requested table is available at BigQuery to be downloaded

    TODO(refactor) remove the population step from this function. Structure is first to be created and then populated
        iteratively with many HTTP Archive tables of custom selection
    """
    print("##########...Preparing temporary infrastructure for NEL analysis data...##########")

    # Build a BigQuery infrastructure administration client instance
    client = _bq_infrastructure_administration_client()

    # Prepare infrastructure for working NEL data
    _create_temp_dataset(client)
    _create_temp_table(client)

    # Populate the prepared table with NEL data
    _populate_temp_table(client)


def _bq_infrastructure_administration_client() -> bigquery.Client:
    """
    Create a simple BigQuery client able to handle infrastructure administration.
    Note that the client needs valid credentials with just enough privileges to create datasets,
    tables and execute query jobs.

    :return: BigQuery client meant to handle infrastructure administration
    """
    credentials = service_account.Credentials.from_service_account_file(
        GC_PATH_TO_CREDENTIALS_FILE,
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )
    client = bigquery.Client(credentials=credentials, project=GC_PROJECT_NAME)

    return client


def _create_temp_dataset(client):
    """
    Creates a BigQuery dataset with a pre-configured name (see global config variables - {GC_DATASET_NAME})
    """
    dataset_id = f"{client.project}.{GC_DATASET_NAME}"
    dataset = bigquery.Dataset(dataset_id)

    one_day_in_ms = 1000 * 60 * 60 * 24

    dataset.location = "US"
    dataset.description = "Dataset for NEL Analysis data"
    dataset.default_table_expiration_ms = one_day_in_ms

    try:
        dataset = client.create_dataset(dataset, timeout=30)
        print(f"Created DATASET '{client.project}.{dataset.dataset_id}'")

    except google.api_core.exceptions.Conflict:
        print(f"DATASET '{client.project}.{dataset.dataset_id}' already exists. Proceeding to use it")


def _create_temp_table(client: bigquery.Client):
    """
    Create a temporary BigQuery table for storing intermediate query results of data for the NEL analysis.
    If the temporary table already exists, delete all it's contents before using it.

    NOTE: It is required, that this method is called after the dataset of name {GC_DATASET_NAME}
          has already been created

    :param client: Basic Google Cloud BigQuery API client with credentials and project ID
                   already provided
    """
    table_id = f"{GC_PROJECT_NAME}.{GC_DATASET_NAME}.{GC_TABLE_NAME}"

    nel_data_schema = [
        bigquery.SchemaField("requestId", "INTEGER"),
        bigquery.SchemaField("firstReq", "BOOLEAN"),
        bigquery.SchemaField("type", "STRING"),
        bigquery.SchemaField("ext", "STRING"),
        bigquery.SchemaField("url", "STRING"),
        bigquery.SchemaField("status", "INTEGER"),
        bigquery.SchemaField("total_crawled_resources", "INTEGER"),
        bigquery.SchemaField("total_crawled_domains", "INTEGER"),
        bigquery.SchemaField("nel_max_age", "STRING"),
        bigquery.SchemaField("nel_failure_fraction", "STRING"),
        bigquery.SchemaField("nel_success_fraction", "STRING"),
        bigquery.SchemaField("nel_include_subdomains", "STRING"),
        bigquery.SchemaField("nel_report_to", "STRING"),
        bigquery.SchemaField("total_crawled_resources_with_nel", "INTEGER"),
        bigquery.SchemaField("rt_group", "STRING"),
        bigquery.SchemaField("rt_collectors", "STRING", mode="REPEATED"),
    ]

    table = bigquery.Table(table_id, schema=nel_data_schema)

    try:
        table = client.create_table(table)  # Make an API request.
        print(f"Created temporary TABLE '{table.project}.{table.dataset_id}.{table.table_id}'")

    except google.api_core.exceptions.Conflict:
        print(f"Temporary TABLE '{table.project}.{table.dataset_id}.{table.table_id}' already exists")

        # NOTE: billing needs to be enabled for the project in order for the DELETE OP to work
        delete_table_contents_query = f"DELETE FROM `{table_id}` WHERE 1=1;"
        client.query_and_wait(delete_table_contents_query)
        print("Temporary TABLE contents deleted. Proceeding to use it")


def _populate_temp_table(client: bigquery.Client):
    """
    Populate the temporary BigQuery table (created by this script) with data fetched from the configured
    HTTP Archive table using the query string provided as a variable in the global scope of this script.

    :param client: Basic Google Cloud BigQuery API client with credentials and project ID
                   already provided
    """
    target_table_id = f"{GC_PROJECT_NAME}.{GC_DATASET_NAME}.{GC_TABLE_NAME}"

    job_config = bigquery.QueryJobConfig(destination=target_table_id)
    client.query_and_wait(DATA_PREPARATION_NEL_DATA_QUERY % DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE, job_config=job_config)

    print(f"Loaded NEL analysis data from '{DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE}' to the temporary table '{target_table_id}'")


def download_nel_data_table():
    """
    Fetch potentially large sized data from the temporary BigQuery table (managed by this script).
    This function uses different API for the data transfer - Google Cloud BigQuery Storage API, which allows for the
    large sized data to be transferred from BigQuery.

    TODO(costs) ask for user's permission for every table, check also for the table already being in the target dir
    TODO(cleanup) this is probably not needed anymore
    """

    print("####################...Downloading NEL data...####################")

    client = _bq_storage_read_session_client()

    table_path = f"projects/{GC_PROJECT_NAME}/datasets/{GC_DATASET_NAME}/tables/{GC_TABLE_NAME}"

    requested_session = types.ReadSession()
    requested_session.table = table_path
    requested_session.data_format = types.DataFormat.AVRO

    parent_project = f"projects/{GC_PROJECT_NAME}"

    session = client.create_read_session(
        parent=parent_project,
        read_session=requested_session,
        max_stream_count=1
    )

    # TODO consider leveraging multiple streams to speed up the process of downloading data
    reader = client.read_rows(session.streams[0].name)
    rows = reader.rows(session)

    data = []
    for row in rows:
        # TODO large datasets will take a long time to download while this solution will keep on eating RAM..
        #      Consider optimizing
        data.append(row)

    df = pd.DataFrame(data)
    df.to_parquet(f"./data_http_archive_raw/{DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE}.parquet")

    print("NEL data downloaded successfully")


def _bq_storage_read_session_client():
    """
    Create a BigQuery Storage API client able to read large sum of data from BigQuery tables
    """
    return BigQueryReadClient.from_service_account_json(GC_PATH_TO_CREDENTIALS_FILE)


def delete_nel_data_table():
    print("####################...Deleting NEL data...####################")
    raise NotImplementedError()


def export_nel_data_table():
    print("####################...Extracting NEL data...####################")
    client = _bq_infrastructure_administration_client()

    destination_uri = "gs://{}/{}".format(DATA_EXPORT_BUCKET_NAME,
                                          f"{DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE}-*.parquet.snappy")
    dataset_ref = bigquery.DatasetReference(GC_PROJECT_NAME, GC_DATASET_NAME)
    table_ref = dataset_ref.table(GC_TABLE_NAME)

    job_config = bigquery.job.ExtractJobConfig()
    job_config.destination_format = bigquery.DestinationFormat.PARQUET
    job_config.compression = bigquery.Compression.SNAPPY

    extract_job = client.extract_table(
        table_ref,
        destination_uri,
        location=GC_TABLE_LOCATION,
        job_config=job_config,
    )

    extract_job.result()  # Waits for job to complete.

    print("NEL data exported successfully and is ready to be downloaded")


def download_exported_nel_data():
    print("####################...Downloading exported NEL data...####################")
    storage_client = storage.Client.from_service_account_json(GC_PATH_TO_CREDENTIALS_FILE)
    bucket = storage_client.get_bucket(DATA_EXPORT_BUCKET_NAME)
    blobs = bucket.list_blobs(prefix=DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE)

    dl_time = time.time()

    # TODO make sure the output dir exists
    for blob in blobs:
        destination_uri = '{}/{}'.format("data_http_archive_raw/gs_test", blob.name)
        blob.download_to_filename(destination_uri)

    print("Exported NEL data downloaded successfully")
    print(f"Download time: {time.time() - dl_time}")


def gather_downloaded_nel_data_files():
    print("####################...Gathering downloaded NEL data into a single file...####################")
    gather_time = time.time()

    data_dir = Path('data_http_archive_raw/gs_test')
    files = list(data_dir.glob('*.parquet.snappy'))

    if len(files) < 1:
        print("Download completed with 0 files downloaded, aborting gathering downloaded NEL data into a single file")
        return

    schema = pq.ParquetFile(files[0]).schema_arrow
    with pq.ParquetWriter("data_http_archive_raw/gs_test_result.parquet", schema=schema) as writer:
        for parquet_file in files:
            writer.write_table(pq.read_table(parquet_file, schema=schema))

    print()
    print(f"Result filesize: {os.path.getsize('data_http_archive_raw/gs_test_result.parquet') / 2 ** 30} GB")
    print(f"Gather time time: {time.time() - gather_time}")


def delete_exported_nel_data():
    print("####################...Deleting exported NEL data...####################")
    storage_client = storage.Client.from_service_account_json(GC_PATH_TO_CREDENTIALS_FILE)
    bucket = storage_client.get_bucket(DATA_EXPORT_BUCKET_NAME)
    blobs = bucket.list_blobs(prefix=DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE)

    for blob in blobs:
        blob.delete()

    print("Exported NEL data deleted successfully")


def main():
    # prepare_nel_data_table()
    # print()

    # export_nel_data_table()
    # print()

    # download_exported_nel_data()
    # print()

    gather_downloaded_nel_data_files()
    print()

    # delete_exported_nel_data()
    # print()

    # TODO consider
    # download_nel_data_table()
    # print()

    # delete_nel_data_table()

    pass


if '__main__' == __name__:
    main()
