#!/usr/bin/env python3
"""
author:         Matej Jurík <xjurik12@stud.fit.vutbr.cz>

description:    Query the Google Cloud Platform's BigQuery for HTTP Archive datasets containing summary of requests
                and their responses generated by crawling the internet.
                The specific query being run here extracts all responses that returned with a NEL header.

                This script acts as the updated version of the former script created by Kamil Jeřábek
                (original repo: from https://github.com/kjerabek/nel-http-archive-analysis).
                The necessity of this script comes from the need to transfer a large volume of data from BigQuery.
                The former script somehow managed to query the HTTP Archive datasets via the base interface
                (google.cloud.bigquery API) without reaching its limits for the maximum size of each query results.
                Here, the BigQuery Storage API (google.cloud.bigquery_storage) is used to retrieve large volume of data.

purpose:        Get raw data from the Google Cloud and store as an apache parquet file in order for it to be available
                locally for further processing.
                (IMPORTANT: do not modify the downloaded data itself - additional queries cost money)
"""

# TODO make download size estimates


import google.api_core.exceptions
import pandas as pd
from google.cloud import bigquery
from google.cloud.bigquery_storage import BigQueryReadClient
from google.cloud.bigquery_storage_v1 import types
from google.oauth2 import service_account

from src.bq_parametrized_queries import QUERY_NEL_VALUE_WITHOUT_FILTERING


###############################
# CONFIGURE THESE BEFORE USE: #
###############################
GC_PROJECT_NAME = 'nel-analysis'
GC_DATASET_NAME = 'httparchive_fetch_temp'
GC_TABLE_NAME = 'nel_data'

GC_PATH_TO_CREDENTIALS_FILE = "gcp-secrets/nel-analysis-9e204ca4bd59-admin.json"

# TODO get the eTLD with NET.PUBLIC_SUFFIX(url)
DATA_PREPARATION_NEL_DATA_QUERY = QUERY_NEL_VALUE_WITHOUT_FILTERING
# TODO Elaborate on the data structure used for them tables to download
# TODO external config for the analysis-wide used tables
DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE = 'httparchive.summary_requests.2016_02_15_mobile'


def prepare_nel_data_table():
    """
    Prepare required infrastructure inside BigQuery.
    Create a temporary dataset, a temporary table inside of it and eventually also query HTTP Archive data into it.
    After this function completes successfully, the requested table is available at BigQuery to be downloaded

    TODO remove the population step from this function. Structure is first to be created and then populated iteratively
         with many HTTP Archive tables of custom selection
    """
    print("##########...Preparing temporary infrastructure for NEL analysis data...##########")

    # Build a BigQuery infrastructure administration client instance
    client = _bq_infrastructure_administration_client()

    # Prepare infrastructure for working NEL data
    _create_temp_dataset(client)
    _create_temp_table(client)

    # Populate the prepared table with NEL data
    _populate_temp_table(client)


def _bq_infrastructure_administration_client() -> bigquery.Client:
    """
    Create a simple BigQuery client able to handle infrastructure administration.
    Note that the client needs valid credentials with just enough privileges to create datasets,
    tables and execute query jobs.

    :return: BigQuery client meant to handle infrastructure administration
    """
    credentials = service_account.Credentials.from_service_account_file(
        GC_PATH_TO_CREDENTIALS_FILE,
        scopes=["https://www.googleapis.com/auth/cloud-platform"]
    )
    client = bigquery.Client(credentials=credentials, project=GC_PROJECT_NAME)

    return client


def _create_temp_dataset(client):
    """
    Creates a BigQuery dataset with a pre-configured name (see global config variables - {GC_DATASET_NAME})
    """
    dataset_id = f"{client.project}.{GC_DATASET_NAME}"
    dataset = bigquery.Dataset(dataset_id)

    one_day_in_ms = 1000 * 60 * 60 * 24

    dataset.location = "US"
    dataset.description = "Dataset for NEL Analysis data"
    dataset.default_table_expiration_ms = one_day_in_ms

    try:
        dataset = client.create_dataset(dataset, timeout=30)
        print(f"Created DATASET '{client.project}.{dataset.dataset_id}'")

    except google.api_core.exceptions.Conflict:
        print(f"DATASET '{client.project}.{dataset.dataset_id}' already exists. Proceeding to use it")


def _create_temp_table(client: bigquery.Client):
    """
    Create a temporary BigQuery table for storing intermediate query results of data for the NEL analysis.
    If the temporary table already exists, delete all it's contents before using it.

    NOTE: It is required, that this method is called after the dataset of name {GC_DATASET_NAME}
          has already been created

    :param client: Basic Google Cloud BigQuery API client with credentials and project ID
                   already provided
    """
    table_id = f"{GC_PROJECT_NAME}.{GC_DATASET_NAME}.{GC_TABLE_NAME}"

    nel_data_schema = [
        bigquery.SchemaField("requestId", "INTEGER"),
        bigquery.SchemaField("firstReq", "BOOLEAN"),
        bigquery.SchemaField("type", "STRING"),
        bigquery.SchemaField("ext", "STRING"),
        bigquery.SchemaField("url", "STRING"),
        bigquery.SchemaField("status", "INTEGER"),
        bigquery.SchemaField("total_crawled_resources", "INTEGER"),
        bigquery.SchemaField("total_crawled_domains", "INTEGER"),
        bigquery.SchemaField("nel_max_age", "STRING"),
        bigquery.SchemaField("nel_failure_fraction", "STRING"),
        bigquery.SchemaField("nel_success_fraction", "STRING"),
        bigquery.SchemaField("nel_include_subdomains", "STRING"),
        bigquery.SchemaField("nel_report_to", "STRING"),
        bigquery.SchemaField("total_crawled_resources_with_nel", "INTEGER"),
        bigquery.SchemaField("rt_group", "STRING"),
        bigquery.SchemaField("rt_collectors", "STRING", mode="REPEATED"),
    ]

    table = bigquery.Table(table_id, schema=nel_data_schema)

    try:
        table = client.create_table(table)  # Make an API request.
        print(f"Created temporary TABLE '{table.project}.{table.dataset_id}.{table.table_id}'")

    except google.api_core.exceptions.Conflict:
        print(f"Temporary TABLE '{table.project}.{table.dataset_id}.{table.table_id}' already exists")

        # NOTE: billing needs to be enabled for the project in order for the DELETE OP to work
        delete_table_contents_query = f"DELETE FROM `{table_id}` WHERE 1=1;"
        client.query_and_wait(delete_table_contents_query)
        print("Temporary TABLE contents deleted. Proceeding to use it")


def _populate_temp_table(client: bigquery.Client):
    """
    Populate the temporary BigQuery table (created by this script) with data fetched from the configured
    HTTP Archive table using the query string provided as a variable in the global scope of this script.

    :param client: Basic Google Cloud BigQuery API client with credentials and project ID
                   already provided
    """
    target_table_id = f"{GC_PROJECT_NAME}.{GC_DATASET_NAME}.{GC_TABLE_NAME}"

    job_config = bigquery.QueryJobConfig(destination=target_table_id)
    client.query_and_wait(DATA_PREPARATION_NEL_DATA_QUERY % DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE, job_config=job_config)

    print(f"Loaded NEL analysis data from '{DATA_PREPARATION_HTTP_ARCHIVE_SOURCE_TABLE}' to the temporary table '{target_table_id}'")


def download_nel_data_table():
    """
    Fetch potentially large sized data from the temporary BigQuery table (managed by this script).
    This function uses different API for the data transfer - Google Cloud BigQuery Storage API, which allows for the
    large sized data to be transferred from BigQuery.

    TODO ask for user's permission for every table, check also for the table already being in the target dir
    """

    print("####################...Downloading NEL data...####################")

    client = _bq_storage_read_session_client()

    table_path = f"projects/{GC_PROJECT_NAME}/datasets/{GC_DATASET_NAME}/tables/{GC_TABLE_NAME}"

    requested_session = types.ReadSession()
    requested_session.table = table_path
    requested_session.data_format = types.DataFormat.AVRO

    parent_project = f"projects/{GC_PROJECT_NAME}"

    session = client.create_read_session(
        parent=parent_project,
        read_session=requested_session,
        max_stream_count=1
    )

    # TODO consider leveraging multiple streams to speed up the process of downloading data
    reader = client.read_rows(session.streams[0].name)
    rows = reader.rows(session)

    data = []
    for row in rows:
        # TODO large datasets will take a long time to download while this solution will keep on eating RAM..
        #      Consider optimizing
        data.append(row)

    df = pd.DataFrame(data)
    df.to_parquet("./data_http_archive_raw/temp/test.parquet")


def _bq_storage_read_session_client():
    """
    Create a BigQuery Storage API client able to read large sum of data from BigQuery tables
    """
    return BigQueryReadClient.from_service_account_json(GC_PATH_TO_CREDENTIALS_FILE)


def delete_nel_data_table():
    print("####################...Deleting NEL data...####################")
    raise NotImplementedError()


def main():
    # prepare_nel_data_table()
    # print()

    # download_nel_data_table()
    # print()

    # TODO consider
    # delete_nel_data_table()

    pass


if '__main__' == __name__:
    main()
